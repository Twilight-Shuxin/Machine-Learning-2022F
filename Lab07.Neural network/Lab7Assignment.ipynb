{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ-w0ikXrvEV"
   },
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Di1fucS4rW2u",
    "outputId": "245bac29-2457-4792-976b-f0017f96fd85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "0   1   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "1   0   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "2   1   1   0   1   0   1   1   0   1   1   1   1   0\n",
       "3   1   1   1   1   0   1   1   0   1   1   1   0   0\n",
       "4   1   1   1   1   0   1   1   0   1   0   1   1   0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"./datasets/digit01.csv\", header=None)\n",
    "df.head()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wjFdbqNOsTfP"
   },
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "y = df[12]\n",
    "X = df.drop(12, axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n",
    "\n",
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dcYNrhdesW95"
   },
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn. Module.\n",
    "########### Write Your Code Here ###########\n",
    "import  torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_i, n_o):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_i, n_o)\n",
    "    def forward(self, input):\n",
    "        return torch.sigmoid(self.linear(input))\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BPamycnbvFEd"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = LogisticRegression(X_train.shape[1],y_train.unique().size()[0] - 1)\n",
    "if torch.cuda.is_available():\n",
    "    models.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTNf3Mnpxf1F"
   },
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2edEbAPKxciH"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ0OiAoYxgvt"
   },
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ds1_uR38xgZh"
   },
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_w9Cb7vxn6Y",
    "outputId": "6ff5fcee-82d6-4651-d9b5-10f7bc8cfff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 0.8808\n",
      "epoch: 40, loss = 0.8705\n",
      "epoch: 60, loss = 0.8604\n",
      "epoch: 80, loss = 0.8507\n",
      "epoch: 100, loss = 0.8411\n",
      "epoch: 120, loss = 0.8318\n",
      "epoch: 140, loss = 0.8228\n",
      "epoch: 160, loss = 0.8139\n",
      "epoch: 180, loss = 0.8053\n",
      "epoch: 200, loss = 0.7969\n",
      "epoch: 220, loss = 0.7888\n",
      "epoch: 240, loss = 0.7808\n",
      "epoch: 260, loss = 0.7730\n",
      "epoch: 280, loss = 0.7654\n",
      "epoch: 300, loss = 0.7580\n",
      "epoch: 320, loss = 0.7508\n",
      "epoch: 340, loss = 0.7437\n",
      "epoch: 360, loss = 0.7368\n",
      "epoch: 380, loss = 0.7301\n",
      "epoch: 400, loss = 0.7235\n",
      "epoch: 420, loss = 0.7171\n",
      "epoch: 440, loss = 0.7108\n",
      "epoch: 460, loss = 0.7046\n",
      "epoch: 480, loss = 0.6986\n",
      "epoch: 500, loss = 0.6927\n",
      "epoch: 520, loss = 0.6869\n",
      "epoch: 540, loss = 0.6813\n",
      "epoch: 560, loss = 0.6758\n",
      "epoch: 580, loss = 0.6704\n",
      "epoch: 600, loss = 0.6651\n",
      "epoch: 620, loss = 0.6599\n",
      "epoch: 640, loss = 0.6548\n",
      "epoch: 660, loss = 0.6498\n",
      "epoch: 680, loss = 0.6448\n",
      "epoch: 700, loss = 0.6400\n",
      "epoch: 720, loss = 0.6353\n",
      "epoch: 740, loss = 0.6306\n",
      "epoch: 760, loss = 0.6261\n",
      "epoch: 780, loss = 0.6216\n",
      "epoch: 800, loss = 0.6172\n",
      "epoch: 820, loss = 0.6129\n",
      "epoch: 840, loss = 0.6086\n",
      "epoch: 860, loss = 0.6044\n",
      "epoch: 880, loss = 0.6003\n",
      "epoch: 900, loss = 0.5962\n",
      "epoch: 920, loss = 0.5922\n",
      "epoch: 940, loss = 0.5883\n",
      "epoch: 960, loss = 0.5844\n",
      "epoch: 980, loss = 0.5806\n",
      "epoch: 1000, loss = 0.5769\n",
      "epoch: 1020, loss = 0.5732\n",
      "epoch: 1040, loss = 0.5695\n",
      "epoch: 1060, loss = 0.5659\n",
      "epoch: 1080, loss = 0.5624\n",
      "epoch: 1100, loss = 0.5589\n",
      "epoch: 1120, loss = 0.5554\n",
      "epoch: 1140, loss = 0.5520\n",
      "epoch: 1160, loss = 0.5487\n",
      "epoch: 1180, loss = 0.5454\n",
      "epoch: 1200, loss = 0.5421\n",
      "epoch: 1220, loss = 0.5389\n",
      "epoch: 1240, loss = 0.5357\n",
      "epoch: 1260, loss = 0.5325\n",
      "epoch: 1280, loss = 0.5294\n",
      "epoch: 1300, loss = 0.5264\n",
      "epoch: 1320, loss = 0.5233\n",
      "epoch: 1340, loss = 0.5203\n",
      "epoch: 1360, loss = 0.5174\n",
      "epoch: 1380, loss = 0.5144\n",
      "epoch: 1400, loss = 0.5116\n",
      "epoch: 1420, loss = 0.5087\n",
      "epoch: 1440, loss = 0.5059\n",
      "epoch: 1460, loss = 0.5031\n",
      "epoch: 1480, loss = 0.5003\n",
      "epoch: 1500, loss = 0.4976\n",
      "epoch: 1520, loss = 0.4949\n",
      "epoch: 1540, loss = 0.4922\n",
      "epoch: 1560, loss = 0.4896\n",
      "epoch: 1580, loss = 0.4870\n",
      "epoch: 1600, loss = 0.4844\n",
      "epoch: 1620, loss = 0.4818\n",
      "epoch: 1640, loss = 0.4793\n",
      "epoch: 1660, loss = 0.4768\n",
      "epoch: 1680, loss = 0.4743\n",
      "epoch: 1700, loss = 0.4719\n",
      "epoch: 1720, loss = 0.4694\n",
      "epoch: 1740, loss = 0.4670\n",
      "epoch: 1760, loss = 0.4647\n",
      "epoch: 1780, loss = 0.4623\n",
      "epoch: 1800, loss = 0.4600\n",
      "epoch: 1820, loss = 0.4577\n",
      "epoch: 1840, loss = 0.4554\n",
      "epoch: 1860, loss = 0.4531\n",
      "epoch: 1880, loss = 0.4509\n",
      "epoch: 1900, loss = 0.4487\n",
      "epoch: 1920, loss = 0.4465\n",
      "epoch: 1940, loss = 0.4443\n",
      "epoch: 1960, loss = 0.4422\n",
      "epoch: 1980, loss = 0.4400\n",
      "epoch: 2000, loss = 0.4379\n",
      "epoch: 2020, loss = 0.4358\n",
      "epoch: 2040, loss = 0.4337\n",
      "epoch: 2060, loss = 0.4317\n",
      "epoch: 2080, loss = 0.4297\n",
      "epoch: 2100, loss = 0.4276\n",
      "epoch: 2120, loss = 0.4256\n",
      "epoch: 2140, loss = 0.4237\n",
      "epoch: 2160, loss = 0.4217\n",
      "epoch: 2180, loss = 0.4198\n",
      "epoch: 2200, loss = 0.4178\n",
      "epoch: 2220, loss = 0.4159\n",
      "epoch: 2240, loss = 0.4140\n",
      "epoch: 2260, loss = 0.4122\n",
      "epoch: 2280, loss = 0.4103\n",
      "epoch: 2300, loss = 0.4085\n",
      "epoch: 2320, loss = 0.4066\n",
      "epoch: 2340, loss = 0.4048\n",
      "epoch: 2360, loss = 0.4030\n",
      "epoch: 2380, loss = 0.4013\n",
      "epoch: 2400, loss = 0.3995\n",
      "epoch: 2420, loss = 0.3978\n",
      "epoch: 2440, loss = 0.3960\n",
      "epoch: 2460, loss = 0.3943\n",
      "epoch: 2480, loss = 0.3926\n",
      "epoch: 2500, loss = 0.3909\n",
      "epoch: 2520, loss = 0.3893\n",
      "epoch: 2540, loss = 0.3876\n",
      "epoch: 2560, loss = 0.3859\n",
      "epoch: 2580, loss = 0.3843\n",
      "epoch: 2600, loss = 0.3827\n",
      "epoch: 2620, loss = 0.3811\n",
      "epoch: 2640, loss = 0.3795\n",
      "epoch: 2660, loss = 0.3779\n",
      "epoch: 2680, loss = 0.3764\n",
      "epoch: 2700, loss = 0.3748\n",
      "epoch: 2720, loss = 0.3733\n",
      "epoch: 2740, loss = 0.3718\n",
      "epoch: 2760, loss = 0.3702\n",
      "epoch: 2780, loss = 0.3687\n",
      "epoch: 2800, loss = 0.3673\n",
      "epoch: 2820, loss = 0.3658\n",
      "epoch: 2840, loss = 0.3643\n",
      "epoch: 2860, loss = 0.3629\n",
      "epoch: 2880, loss = 0.3614\n",
      "epoch: 2900, loss = 0.3600\n",
      "epoch: 2920, loss = 0.3586\n",
      "epoch: 2940, loss = 0.3572\n",
      "epoch: 2960, loss = 0.3558\n",
      "epoch: 2980, loss = 0.3544\n",
      "epoch: 3000, loss = 0.3530\n",
      "epoch: 3020, loss = 0.3516\n",
      "epoch: 3040, loss = 0.3503\n",
      "epoch: 3060, loss = 0.3490\n",
      "epoch: 3080, loss = 0.3476\n",
      "epoch: 3100, loss = 0.3463\n",
      "epoch: 3120, loss = 0.3450\n",
      "epoch: 3140, loss = 0.3437\n",
      "epoch: 3160, loss = 0.3424\n",
      "epoch: 3180, loss = 0.3411\n",
      "epoch: 3200, loss = 0.3398\n",
      "epoch: 3220, loss = 0.3386\n",
      "epoch: 3240, loss = 0.3373\n",
      "epoch: 3260, loss = 0.3361\n",
      "epoch: 3280, loss = 0.3349\n",
      "epoch: 3300, loss = 0.3336\n",
      "epoch: 3320, loss = 0.3324\n",
      "epoch: 3340, loss = 0.3312\n",
      "epoch: 3360, loss = 0.3300\n",
      "epoch: 3380, loss = 0.3288\n",
      "epoch: 3400, loss = 0.3277\n",
      "epoch: 3420, loss = 0.3265\n",
      "epoch: 3440, loss = 0.3253\n",
      "epoch: 3460, loss = 0.3242\n",
      "epoch: 3480, loss = 0.3230\n",
      "epoch: 3500, loss = 0.3219\n",
      "epoch: 3520, loss = 0.3208\n",
      "epoch: 3540, loss = 0.3196\n",
      "epoch: 3560, loss = 0.3185\n",
      "epoch: 3580, loss = 0.3174\n",
      "epoch: 3600, loss = 0.3163\n",
      "epoch: 3620, loss = 0.3152\n",
      "epoch: 3640, loss = 0.3142\n",
      "epoch: 3660, loss = 0.3131\n",
      "epoch: 3680, loss = 0.3120\n",
      "epoch: 3700, loss = 0.3110\n",
      "epoch: 3720, loss = 0.3099\n",
      "epoch: 3740, loss = 0.3089\n",
      "epoch: 3760, loss = 0.3078\n",
      "epoch: 3780, loss = 0.3068\n",
      "epoch: 3800, loss = 0.3058\n",
      "epoch: 3820, loss = 0.3048\n",
      "epoch: 3840, loss = 0.3038\n",
      "epoch: 3860, loss = 0.3028\n",
      "epoch: 3880, loss = 0.3018\n",
      "epoch: 3900, loss = 0.3008\n",
      "epoch: 3920, loss = 0.2998\n",
      "epoch: 3940, loss = 0.2988\n",
      "epoch: 3960, loss = 0.2979\n",
      "epoch: 3980, loss = 0.2969\n",
      "epoch: 4000, loss = 0.2960\n",
      "epoch: 4020, loss = 0.2950\n",
      "epoch: 4040, loss = 0.2941\n",
      "epoch: 4060, loss = 0.2931\n",
      "epoch: 4080, loss = 0.2922\n",
      "epoch: 4100, loss = 0.2913\n",
      "epoch: 4120, loss = 0.2904\n",
      "epoch: 4140, loss = 0.2895\n",
      "epoch: 4160, loss = 0.2886\n",
      "epoch: 4180, loss = 0.2877\n",
      "epoch: 4200, loss = 0.2868\n",
      "epoch: 4220, loss = 0.2859\n",
      "epoch: 4240, loss = 0.2850\n",
      "epoch: 4260, loss = 0.2841\n",
      "epoch: 4280, loss = 0.2833\n",
      "epoch: 4300, loss = 0.2824\n",
      "epoch: 4320, loss = 0.2816\n",
      "epoch: 4340, loss = 0.2807\n",
      "epoch: 4360, loss = 0.2799\n",
      "epoch: 4380, loss = 0.2790\n",
      "epoch: 4400, loss = 0.2782\n",
      "epoch: 4420, loss = 0.2774\n",
      "epoch: 4440, loss = 0.2765\n",
      "epoch: 4460, loss = 0.2757\n",
      "epoch: 4480, loss = 0.2749\n",
      "epoch: 4500, loss = 0.2741\n",
      "epoch: 4520, loss = 0.2733\n",
      "epoch: 4540, loss = 0.2725\n",
      "epoch: 4560, loss = 0.2717\n",
      "epoch: 4580, loss = 0.2709\n",
      "epoch: 4600, loss = 0.2701\n",
      "epoch: 4620, loss = 0.2693\n",
      "epoch: 4640, loss = 0.2686\n",
      "epoch: 4660, loss = 0.2678\n",
      "epoch: 4680, loss = 0.2670\n",
      "epoch: 4700, loss = 0.2663\n",
      "epoch: 4720, loss = 0.2655\n",
      "epoch: 4740, loss = 0.2648\n",
      "epoch: 4760, loss = 0.2640\n",
      "epoch: 4780, loss = 0.2633\n",
      "epoch: 4800, loss = 0.2626\n",
      "epoch: 4820, loss = 0.2618\n",
      "epoch: 4840, loss = 0.2611\n",
      "epoch: 4860, loss = 0.2604\n",
      "epoch: 4880, loss = 0.2597\n",
      "epoch: 4900, loss = 0.2589\n",
      "epoch: 4920, loss = 0.2582\n",
      "epoch: 4940, loss = 0.2575\n",
      "epoch: 4960, loss = 0.2568\n",
      "epoch: 4980, loss = 0.2561\n",
      "epoch: 5000, loss = 0.2554\n",
      "epoch: 5020, loss = 0.2547\n",
      "epoch: 5040, loss = 0.2541\n",
      "epoch: 5060, loss = 0.2534\n",
      "epoch: 5080, loss = 0.2527\n",
      "epoch: 5100, loss = 0.2520\n",
      "epoch: 5120, loss = 0.2514\n",
      "epoch: 5140, loss = 0.2507\n",
      "epoch: 5160, loss = 0.2500\n",
      "epoch: 5180, loss = 0.2494\n",
      "epoch: 5200, loss = 0.2487\n",
      "epoch: 5220, loss = 0.2481\n",
      "epoch: 5240, loss = 0.2474\n",
      "epoch: 5260, loss = 0.2468\n",
      "epoch: 5280, loss = 0.2462\n",
      "epoch: 5300, loss = 0.2455\n",
      "epoch: 5320, loss = 0.2449\n",
      "epoch: 5340, loss = 0.2443\n",
      "epoch: 5360, loss = 0.2436\n",
      "epoch: 5380, loss = 0.2430\n",
      "epoch: 5400, loss = 0.2424\n",
      "epoch: 5420, loss = 0.2418\n",
      "epoch: 5440, loss = 0.2412\n",
      "epoch: 5460, loss = 0.2406\n",
      "epoch: 5480, loss = 0.2400\n",
      "epoch: 5500, loss = 0.2394\n",
      "epoch: 5520, loss = 0.2388\n",
      "epoch: 5540, loss = 0.2382\n",
      "epoch: 5560, loss = 0.2376\n",
      "epoch: 5580, loss = 0.2370\n",
      "epoch: 5600, loss = 0.2364\n",
      "epoch: 5620, loss = 0.2358\n",
      "epoch: 5640, loss = 0.2353\n",
      "epoch: 5660, loss = 0.2347\n",
      "epoch: 5680, loss = 0.2341\n",
      "epoch: 5700, loss = 0.2335\n",
      "epoch: 5720, loss = 0.2330\n",
      "epoch: 5740, loss = 0.2324\n",
      "epoch: 5760, loss = 0.2319\n",
      "epoch: 5780, loss = 0.2313\n",
      "epoch: 5800, loss = 0.2307\n",
      "epoch: 5820, loss = 0.2302\n",
      "epoch: 5840, loss = 0.2296\n",
      "epoch: 5860, loss = 0.2291\n",
      "epoch: 5880, loss = 0.2286\n",
      "epoch: 5900, loss = 0.2280\n",
      "epoch: 5920, loss = 0.2275\n",
      "epoch: 5940, loss = 0.2270\n",
      "epoch: 5960, loss = 0.2264\n",
      "epoch: 5980, loss = 0.2259\n",
      "epoch: 6000, loss = 0.2254\n",
      "epoch: 6020, loss = 0.2249\n",
      "epoch: 6040, loss = 0.2243\n",
      "epoch: 6060, loss = 0.2238\n",
      "epoch: 6080, loss = 0.2233\n",
      "epoch: 6100, loss = 0.2228\n",
      "epoch: 6120, loss = 0.2223\n",
      "epoch: 6140, loss = 0.2218\n",
      "epoch: 6160, loss = 0.2213\n",
      "epoch: 6180, loss = 0.2208\n",
      "epoch: 6200, loss = 0.2203\n",
      "epoch: 6220, loss = 0.2198\n",
      "epoch: 6240, loss = 0.2193\n",
      "epoch: 6260, loss = 0.2188\n",
      "epoch: 6280, loss = 0.2183\n",
      "epoch: 6300, loss = 0.2178\n",
      "epoch: 6320, loss = 0.2173\n",
      "epoch: 6340, loss = 0.2169\n",
      "epoch: 6360, loss = 0.2164\n",
      "epoch: 6380, loss = 0.2159\n",
      "epoch: 6400, loss = 0.2154\n",
      "epoch: 6420, loss = 0.2150\n",
      "epoch: 6440, loss = 0.2145\n",
      "epoch: 6460, loss = 0.2140\n",
      "epoch: 6480, loss = 0.2136\n",
      "epoch: 6500, loss = 0.2131\n",
      "epoch: 6520, loss = 0.2126\n",
      "epoch: 6540, loss = 0.2122\n",
      "epoch: 6560, loss = 0.2117\n",
      "epoch: 6580, loss = 0.2113\n",
      "epoch: 6600, loss = 0.2108\n",
      "epoch: 6620, loss = 0.2104\n",
      "epoch: 6640, loss = 0.2099\n",
      "epoch: 6660, loss = 0.2095\n",
      "epoch: 6680, loss = 0.2091\n",
      "epoch: 6700, loss = 0.2086\n",
      "epoch: 6720, loss = 0.2082\n",
      "epoch: 6740, loss = 0.2077\n",
      "epoch: 6760, loss = 0.2073\n",
      "epoch: 6780, loss = 0.2069\n",
      "epoch: 6800, loss = 0.2064\n",
      "epoch: 6820, loss = 0.2060\n",
      "epoch: 6840, loss = 0.2056\n",
      "epoch: 6860, loss = 0.2052\n",
      "epoch: 6880, loss = 0.2048\n",
      "epoch: 6900, loss = 0.2043\n",
      "epoch: 6920, loss = 0.2039\n",
      "epoch: 6940, loss = 0.2035\n",
      "epoch: 6960, loss = 0.2031\n",
      "epoch: 6980, loss = 0.2027\n",
      "epoch: 7000, loss = 0.2023\n",
      "epoch: 7020, loss = 0.2019\n",
      "epoch: 7040, loss = 0.2015\n",
      "epoch: 7060, loss = 0.2011\n",
      "epoch: 7080, loss = 0.2006\n",
      "epoch: 7100, loss = 0.2002\n",
      "epoch: 7120, loss = 0.1999\n",
      "epoch: 7140, loss = 0.1995\n",
      "epoch: 7160, loss = 0.1991\n",
      "epoch: 7180, loss = 0.1987\n",
      "epoch: 7200, loss = 0.1983\n",
      "epoch: 7220, loss = 0.1979\n",
      "epoch: 7240, loss = 0.1975\n",
      "epoch: 7260, loss = 0.1971\n",
      "epoch: 7280, loss = 0.1967\n",
      "epoch: 7300, loss = 0.1963\n",
      "epoch: 7320, loss = 0.1960\n",
      "epoch: 7340, loss = 0.1956\n",
      "epoch: 7360, loss = 0.1952\n",
      "epoch: 7380, loss = 0.1948\n",
      "epoch: 7400, loss = 0.1945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7420, loss = 0.1941\n",
      "epoch: 7440, loss = 0.1937\n",
      "epoch: 7460, loss = 0.1934\n",
      "epoch: 7480, loss = 0.1930\n",
      "epoch: 7500, loss = 0.1926\n",
      "epoch: 7520, loss = 0.1923\n",
      "epoch: 7540, loss = 0.1919\n",
      "epoch: 7560, loss = 0.1915\n",
      "epoch: 7580, loss = 0.1912\n",
      "epoch: 7600, loss = 0.1908\n",
      "epoch: 7620, loss = 0.1905\n",
      "epoch: 7640, loss = 0.1901\n",
      "epoch: 7660, loss = 0.1898\n",
      "epoch: 7680, loss = 0.1894\n",
      "epoch: 7700, loss = 0.1891\n",
      "epoch: 7720, loss = 0.1887\n",
      "epoch: 7740, loss = 0.1884\n",
      "epoch: 7760, loss = 0.1880\n",
      "epoch: 7780, loss = 0.1877\n",
      "epoch: 7800, loss = 0.1873\n",
      "epoch: 7820, loss = 0.1870\n",
      "epoch: 7840, loss = 0.1866\n",
      "epoch: 7860, loss = 0.1863\n",
      "epoch: 7880, loss = 0.1860\n",
      "epoch: 7900, loss = 0.1856\n",
      "epoch: 7920, loss = 0.1853\n",
      "epoch: 7940, loss = 0.1850\n",
      "epoch: 7960, loss = 0.1846\n",
      "epoch: 7980, loss = 0.1843\n",
      "epoch: 8000, loss = 0.1840\n",
      "epoch: 8020, loss = 0.1837\n",
      "epoch: 8040, loss = 0.1833\n",
      "epoch: 8060, loss = 0.1830\n",
      "epoch: 8080, loss = 0.1827\n",
      "epoch: 8100, loss = 0.1824\n",
      "epoch: 8120, loss = 0.1820\n",
      "epoch: 8140, loss = 0.1817\n",
      "epoch: 8160, loss = 0.1814\n",
      "epoch: 8180, loss = 0.1811\n",
      "epoch: 8200, loss = 0.1808\n",
      "epoch: 8220, loss = 0.1805\n",
      "epoch: 8240, loss = 0.1801\n",
      "epoch: 8260, loss = 0.1798\n",
      "epoch: 8280, loss = 0.1795\n",
      "epoch: 8300, loss = 0.1792\n",
      "epoch: 8320, loss = 0.1789\n",
      "epoch: 8340, loss = 0.1786\n",
      "epoch: 8360, loss = 0.1783\n",
      "epoch: 8380, loss = 0.1780\n",
      "epoch: 8400, loss = 0.1777\n",
      "epoch: 8420, loss = 0.1774\n",
      "epoch: 8440, loss = 0.1771\n",
      "epoch: 8460, loss = 0.1768\n",
      "epoch: 8480, loss = 0.1765\n",
      "epoch: 8500, loss = 0.1762\n",
      "epoch: 8520, loss = 0.1759\n",
      "epoch: 8540, loss = 0.1756\n",
      "epoch: 8560, loss = 0.1753\n",
      "epoch: 8580, loss = 0.1750\n",
      "epoch: 8600, loss = 0.1747\n",
      "epoch: 8620, loss = 0.1744\n",
      "epoch: 8640, loss = 0.1742\n",
      "epoch: 8660, loss = 0.1739\n",
      "epoch: 8680, loss = 0.1736\n",
      "epoch: 8700, loss = 0.1733\n",
      "epoch: 8720, loss = 0.1730\n",
      "epoch: 8740, loss = 0.1727\n",
      "epoch: 8760, loss = 0.1725\n",
      "epoch: 8780, loss = 0.1722\n",
      "epoch: 8800, loss = 0.1719\n",
      "epoch: 8820, loss = 0.1716\n",
      "epoch: 8840, loss = 0.1713\n",
      "epoch: 8860, loss = 0.1711\n",
      "epoch: 8880, loss = 0.1708\n",
      "epoch: 8900, loss = 0.1705\n",
      "epoch: 8920, loss = 0.1702\n",
      "epoch: 8940, loss = 0.1700\n",
      "epoch: 8960, loss = 0.1697\n",
      "epoch: 8980, loss = 0.1694\n",
      "epoch: 9000, loss = 0.1692\n",
      "epoch: 9020, loss = 0.1689\n",
      "epoch: 9040, loss = 0.1686\n",
      "epoch: 9060, loss = 0.1684\n",
      "epoch: 9080, loss = 0.1681\n",
      "epoch: 9100, loss = 0.1678\n",
      "epoch: 9120, loss = 0.1676\n",
      "epoch: 9140, loss = 0.1673\n",
      "epoch: 9160, loss = 0.1670\n",
      "epoch: 9180, loss = 0.1668\n",
      "epoch: 9200, loss = 0.1665\n",
      "epoch: 9220, loss = 0.1663\n",
      "epoch: 9240, loss = 0.1660\n",
      "epoch: 9260, loss = 0.1658\n",
      "epoch: 9280, loss = 0.1655\n",
      "epoch: 9300, loss = 0.1652\n",
      "epoch: 9320, loss = 0.1650\n",
      "epoch: 9340, loss = 0.1647\n",
      "epoch: 9360, loss = 0.1645\n",
      "epoch: 9380, loss = 0.1642\n",
      "epoch: 9400, loss = 0.1640\n",
      "epoch: 9420, loss = 0.1637\n",
      "epoch: 9440, loss = 0.1635\n",
      "epoch: 9460, loss = 0.1632\n",
      "epoch: 9480, loss = 0.1630\n",
      "epoch: 9500, loss = 0.1628\n",
      "epoch: 9520, loss = 0.1625\n",
      "epoch: 9540, loss = 0.1623\n",
      "epoch: 9560, loss = 0.1620\n",
      "epoch: 9580, loss = 0.1618\n",
      "epoch: 9600, loss = 0.1615\n",
      "epoch: 9620, loss = 0.1613\n",
      "epoch: 9640, loss = 0.1611\n",
      "epoch: 9660, loss = 0.1608\n",
      "epoch: 9680, loss = 0.1606\n",
      "epoch: 9700, loss = 0.1604\n",
      "epoch: 9720, loss = 0.1601\n",
      "epoch: 9740, loss = 0.1599\n",
      "epoch: 9760, loss = 0.1596\n",
      "epoch: 9780, loss = 0.1594\n",
      "epoch: 9800, loss = 0.1592\n",
      "epoch: 9820, loss = 0.1590\n",
      "epoch: 9840, loss = 0.1587\n",
      "epoch: 9860, loss = 0.1585\n",
      "epoch: 9880, loss = 0.1583\n",
      "epoch: 9900, loss = 0.1580\n",
      "epoch: 9920, loss = 0.1578\n",
      "epoch: 9940, loss = 0.1576\n",
      "epoch: 9960, loss = 0.1574\n",
      "epoch: 9980, loss = 0.1571\n",
      "epoch: 10000, loss = 0.1569\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(torch.squeeze(y_pred), y_train.float())\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eljfb7ur0O2w",
    "outputId": "3d908472-bb36-443b-fce6-cb38a869eade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = torch.squeeze(logits.round())\n",
    "    acc = y_pred.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HnQ74F30o2W",
    "outputId": "fbc2f99c-f798-46e3-d047-3762bdb3a33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.92        13\n",
      "   macro avg       0.93      0.93      0.92        13\n",
      "weighted avg       0.93      0.92      0.92        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Muy0vMym2idM"
   },
   "source": [
    "### Exercise 2  Handwriting recognition with MLP(50 points )\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4Z3LR17_2l7Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_int(bytecode, id):\n",
    "    x = 0\n",
    "    for i in range(4):\n",
    "        x = x * (16 ** 2) + bytecode[id + i]\n",
    "    return x\n",
    "    \n",
    "\n",
    "def read_labels(file_path):\n",
    "    file = open(file_path, \"rb\")\n",
    "    bytecode = list(file.read())\n",
    "    item_num = read_int(bytecode, 4)\n",
    "    labels = np.zeros(shape=(item_num))\n",
    "    for i in range(item_num):\n",
    "        labels[i] = (bytecode[8 + i])\n",
    "    return labels\n",
    "    \n",
    "train_labels = np.asarray(read_labels(\"train-labels.idx1-ubyte\"))\n",
    "test_labels = np.asarray(read_labels(\"test-labels.idx1-ubyte\"))\n",
    "\n",
    "def read_images(file_path):\n",
    "    file = open(file_path, \"rb\")\n",
    "    bytecode = list(file.read())\n",
    "    item_num = read_int(bytecode, 4)\n",
    "    row_num = read_int(bytecode, 8)\n",
    "    column_num = read_int(bytecode, 12)\n",
    "    images = np.zeros(shape=(item_num, row_num * column_num))\n",
    "    id = 16\n",
    "    for i in range(item_num):\n",
    "        for idx in range(row_num):\n",
    "            images[i][idx * column_num : idx * column_num + column_num] = bytecode[id : id + column_num]\n",
    "            id += column_num\n",
    "    return images\n",
    "    \n",
    "    \n",
    "train_images = np.asarray(read_images(\"train-images.idx3-ubyte\"))\n",
    "test_images = np.asarray(read_images(\"test-images.idx3-ubyte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "whV1XAA1Kdcw"
   },
   "outputs": [],
   "source": [
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fky456r9OduD",
    "outputId": "2af3688c-768d-4ef4-83fc-c1b4c8bba4b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  84. 185. 159. 151.  60.  36.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 222. 254. 254. 254. 254. 241. 198. 198.\n",
      " 198. 198. 198. 198. 198. 198. 170.  52.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  67. 114.  72. 114. 163. 227. 254. 225.\n",
      " 254. 254. 254. 250. 229. 254. 254. 140.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.  66.  14.\n",
      "  67.  67.  67.  59.  21. 236. 254. 106.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  83. 253. 209.  18.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.  22. 233. 255.  83.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 129. 254. 238.  44.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  59. 249. 254.  62.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. 133. 254. 187.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   9. 205. 248.  58.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. 126. 254. 182.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  75. 251. 240.  57.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19.\n",
      " 221. 254. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 203.\n",
      " 254. 219.  35.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  38. 254.\n",
      " 254.  77.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 224. 254.\n",
      " 115.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 133. 254. 254.\n",
      "  52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61. 242. 254. 254.\n",
      "  52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 254. 219.\n",
      "  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 207.  18.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wQTG3_CgKqQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "\n",
    "X_train = torch.from_numpy(train_images).float()\n",
    "X_test = torch.from_numpy(test_images).float()\n",
    "y_train = torch.from_numpy(train_labels).long()\n",
    "y_test = torch.from_numpy(test_labels).long()\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "print(\"no\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"yes\")\n",
    "    models.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-HncXk5ToiX",
    "outputId": "8a4ffb82-4d7a-4356-f96d-82c0e18f4799",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.4235\n",
      "epoch: 40, loss = 0.9955\n",
      "epoch: 60, loss = 0.8037\n",
      "epoch: 80, loss = 0.6875\n",
      "epoch: 100, loss = 0.6068\n",
      "epoch: 120, loss = 0.5465\n",
      "epoch: 140, loss = 0.4993\n",
      "epoch: 160, loss = 0.4610\n",
      "epoch: 180, loss = 0.4290\n",
      "epoch: 200, loss = 0.4016\n",
      "epoch: 220, loss = 0.3779\n",
      "epoch: 240, loss = 0.3571\n",
      "epoch: 260, loss = 0.3387\n",
      "epoch: 280, loss = 0.3222\n",
      "epoch: 300, loss = 0.3075\n",
      "epoch: 320, loss = 0.2941\n",
      "epoch: 340, loss = 0.2819\n",
      "epoch: 360, loss = 0.2707\n",
      "epoch: 380, loss = 0.2603\n",
      "epoch: 400, loss = 0.2507\n",
      "epoch: 420, loss = 0.2418\n",
      "epoch: 440, loss = 0.2335\n",
      "epoch: 460, loss = 0.2257\n",
      "epoch: 480, loss = 0.2184\n",
      "epoch: 500, loss = 0.2116\n",
      "epoch: 520, loss = 0.2052\n",
      "epoch: 540, loss = 0.1991\n",
      "epoch: 560, loss = 0.1934\n",
      "epoch: 580, loss = 0.1880\n",
      "epoch: 600, loss = 0.1829\n",
      "epoch: 620, loss = 0.1780\n",
      "epoch: 640, loss = 0.1733\n",
      "epoch: 660, loss = 0.1689\n",
      "epoch: 680, loss = 0.1647\n",
      "epoch: 700, loss = 0.1607\n",
      "epoch: 720, loss = 0.1569\n",
      "epoch: 740, loss = 0.1532\n",
      "epoch: 760, loss = 0.1497\n",
      "epoch: 780, loss = 0.1463\n",
      "epoch: 800, loss = 0.1431\n",
      "epoch: 820, loss = 0.1400\n",
      "epoch: 840, loss = 0.1370\n",
      "epoch: 860, loss = 0.1341\n",
      "epoch: 880, loss = 0.1313\n",
      "epoch: 900, loss = 0.1286\n",
      "epoch: 920, loss = 0.1260\n",
      "epoch: 940, loss = 0.1235\n",
      "epoch: 960, loss = 0.1211\n",
      "epoch: 980, loss = 0.1187\n",
      "epoch: 1000, loss = 0.1164\n",
      "epoch: 1020, loss = 0.1142\n",
      "epoch: 1040, loss = 0.1121\n",
      "epoch: 1060, loss = 0.1100\n",
      "epoch: 1080, loss = 0.1080\n",
      "epoch: 1100, loss = 0.1061\n",
      "epoch: 1120, loss = 0.1042\n",
      "epoch: 1140, loss = 0.1023\n",
      "epoch: 1160, loss = 0.1005\n",
      "epoch: 1180, loss = 0.0988\n",
      "epoch: 1200, loss = 0.0971\n",
      "epoch: 1220, loss = 0.0955\n",
      "epoch: 1240, loss = 0.0939\n",
      "epoch: 1260, loss = 0.0923\n",
      "epoch: 1280, loss = 0.0908\n",
      "epoch: 1300, loss = 0.0893\n",
      "epoch: 1320, loss = 0.0879\n",
      "epoch: 1340, loss = 0.0865\n",
      "epoch: 1360, loss = 0.0851\n",
      "epoch: 1380, loss = 0.0838\n",
      "epoch: 1400, loss = 0.0825\n",
      "epoch: 1420, loss = 0.0812\n",
      "epoch: 1440, loss = 0.0800\n",
      "epoch: 1460, loss = 0.0788\n",
      "epoch: 1480, loss = 0.0776\n",
      "epoch: 1500, loss = 0.0765\n",
      "epoch: 1520, loss = 0.0753\n",
      "epoch: 1540, loss = 0.0742\n",
      "epoch: 1560, loss = 0.0732\n",
      "epoch: 1580, loss = 0.0721\n",
      "epoch: 1600, loss = 0.0711\n",
      "epoch: 1620, loss = 0.0701\n",
      "epoch: 1640, loss = 0.0691\n",
      "epoch: 1660, loss = 0.0681\n",
      "epoch: 1680, loss = 0.0672\n",
      "epoch: 1700, loss = 0.0663\n",
      "epoch: 1720, loss = 0.0654\n",
      "epoch: 1740, loss = 0.0645\n",
      "epoch: 1760, loss = 0.0636\n",
      "epoch: 1780, loss = 0.0628\n",
      "epoch: 1800, loss = 0.0619\n",
      "epoch: 1820, loss = 0.0611\n",
      "epoch: 1840, loss = 0.0603\n",
      "epoch: 1860, loss = 0.0595\n",
      "epoch: 1880, loss = 0.0588\n",
      "epoch: 1900, loss = 0.0580\n",
      "epoch: 1920, loss = 0.0573\n",
      "epoch: 1940, loss = 0.0565\n",
      "epoch: 1960, loss = 0.0558\n",
      "epoch: 1980, loss = 0.0551\n",
      "epoch: 2000, loss = 0.0545\n",
      "epoch: 2020, loss = 0.0538\n",
      "epoch: 2040, loss = 0.0531\n",
      "epoch: 2060, loss = 0.0525\n",
      "epoch: 2080, loss = 0.0518\n",
      "epoch: 2100, loss = 0.0512\n",
      "epoch: 2120, loss = 0.0506\n",
      "epoch: 2140, loss = 0.0500\n",
      "epoch: 2160, loss = 0.0494\n",
      "epoch: 2180, loss = 0.0488\n",
      "epoch: 2200, loss = 0.0482\n",
      "epoch: 2220, loss = 0.0477\n",
      "epoch: 2240, loss = 0.0471\n",
      "epoch: 2260, loss = 0.0466\n",
      "epoch: 2280, loss = 0.0461\n",
      "epoch: 2300, loss = 0.0455\n",
      "epoch: 2320, loss = 0.0450\n",
      "epoch: 2340, loss = 0.0445\n",
      "epoch: 2360, loss = 0.0440\n",
      "epoch: 2380, loss = 0.0435\n",
      "epoch: 2400, loss = 0.0430\n",
      "epoch: 2420, loss = 0.0426\n",
      "epoch: 2440, loss = 0.0421\n",
      "epoch: 2460, loss = 0.0416\n",
      "epoch: 2480, loss = 0.0412\n",
      "epoch: 2500, loss = 0.0408\n",
      "epoch: 2520, loss = 0.0403\n",
      "epoch: 2540, loss = 0.0399\n",
      "epoch: 2560, loss = 0.0395\n",
      "epoch: 2580, loss = 0.0390\n",
      "epoch: 2600, loss = 0.0386\n",
      "epoch: 2620, loss = 0.0382\n",
      "epoch: 2640, loss = 0.0378\n",
      "epoch: 2660, loss = 0.0374\n",
      "epoch: 2680, loss = 0.0371\n",
      "epoch: 2700, loss = 0.0367\n",
      "epoch: 2720, loss = 0.0363\n",
      "epoch: 2740, loss = 0.0359\n",
      "epoch: 2760, loss = 0.0356\n",
      "epoch: 2780, loss = 0.0352\n",
      "epoch: 2800, loss = 0.0349\n",
      "epoch: 2820, loss = 0.0345\n",
      "epoch: 2840, loss = 0.0342\n",
      "epoch: 2860, loss = 0.0338\n",
      "epoch: 2880, loss = 0.0335\n",
      "epoch: 2900, loss = 0.0332\n",
      "epoch: 2920, loss = 0.0329\n",
      "epoch: 2940, loss = 0.0325\n",
      "epoch: 2960, loss = 0.0322\n",
      "epoch: 2980, loss = 0.0319\n",
      "epoch: 3000, loss = 0.0316\n",
      "epoch: 3020, loss = 0.0313\n",
      "epoch: 3040, loss = 0.0310\n",
      "epoch: 3060, loss = 0.0307\n",
      "epoch: 3080, loss = 0.0304\n",
      "epoch: 3100, loss = 0.0301\n",
      "epoch: 3120, loss = 0.0299\n",
      "epoch: 3140, loss = 0.0296\n",
      "epoch: 3160, loss = 0.0293\n",
      "epoch: 3180, loss = 0.0290\n",
      "epoch: 3200, loss = 0.0288\n",
      "epoch: 3220, loss = 0.0285\n",
      "epoch: 3240, loss = 0.0283\n",
      "epoch: 3260, loss = 0.0280\n",
      "epoch: 3280, loss = 0.0277\n",
      "epoch: 3300, loss = 0.0275\n",
      "epoch: 3320, loss = 0.0272\n",
      "epoch: 3340, loss = 0.0270\n",
      "epoch: 3360, loss = 0.0268\n",
      "epoch: 3380, loss = 0.0265\n",
      "epoch: 3400, loss = 0.0263\n",
      "epoch: 3420, loss = 0.0261\n",
      "epoch: 3440, loss = 0.0258\n",
      "epoch: 3460, loss = 0.0256\n",
      "epoch: 3480, loss = 0.0254\n",
      "epoch: 3500, loss = 0.0252\n",
      "epoch: 3520, loss = 0.0249\n",
      "epoch: 3540, loss = 0.0247\n",
      "epoch: 3560, loss = 0.0245\n",
      "epoch: 3580, loss = 0.0243\n",
      "epoch: 3600, loss = 0.0241\n",
      "epoch: 3620, loss = 0.0239\n",
      "epoch: 3640, loss = 0.0237\n",
      "epoch: 3660, loss = 0.0235\n",
      "epoch: 3680, loss = 0.0233\n",
      "epoch: 3700, loss = 0.0231\n",
      "epoch: 3720, loss = 0.0229\n",
      "epoch: 3740, loss = 0.0227\n",
      "epoch: 3760, loss = 0.0225\n",
      "epoch: 3780, loss = 0.0223\n",
      "epoch: 3800, loss = 0.0221\n",
      "epoch: 3820, loss = 0.0220\n",
      "epoch: 3840, loss = 0.0218\n",
      "epoch: 3860, loss = 0.0216\n",
      "epoch: 3880, loss = 0.0214\n",
      "epoch: 3900, loss = 0.0213\n",
      "epoch: 3920, loss = 0.0211\n",
      "epoch: 3940, loss = 0.0209\n",
      "epoch: 3960, loss = 0.0207\n",
      "epoch: 3980, loss = 0.0206\n",
      "epoch: 4000, loss = 0.0204\n",
      "epoch: 4020, loss = 0.0202\n",
      "epoch: 4040, loss = 0.0201\n",
      "epoch: 4060, loss = 0.0199\n",
      "epoch: 4080, loss = 0.0198\n",
      "epoch: 4100, loss = 0.0196\n",
      "epoch: 4120, loss = 0.0195\n",
      "epoch: 4140, loss = 0.0193\n",
      "epoch: 4160, loss = 0.0192\n",
      "epoch: 4180, loss = 0.0190\n",
      "epoch: 4200, loss = 0.0189\n",
      "epoch: 4220, loss = 0.0187\n",
      "epoch: 4240, loss = 0.0186\n",
      "epoch: 4260, loss = 0.0184\n",
      "epoch: 4280, loss = 0.0183\n",
      "epoch: 4300, loss = 0.0182\n",
      "epoch: 4320, loss = 0.0180\n",
      "epoch: 4340, loss = 0.0179\n",
      "epoch: 4360, loss = 0.0178\n",
      "epoch: 4380, loss = 0.0176\n",
      "epoch: 4400, loss = 0.0175\n",
      "epoch: 4420, loss = 0.0174\n",
      "epoch: 4440, loss = 0.0172\n",
      "epoch: 4460, loss = 0.0171\n",
      "epoch: 4480, loss = 0.0170\n",
      "epoch: 4500, loss = 0.0169\n",
      "epoch: 4520, loss = 0.0167\n",
      "epoch: 4540, loss = 0.0166\n",
      "epoch: 4560, loss = 0.0165\n",
      "epoch: 4580, loss = 0.0164\n",
      "epoch: 4600, loss = 0.0163\n",
      "epoch: 4620, loss = 0.0162\n",
      "epoch: 4640, loss = 0.0160\n",
      "epoch: 4660, loss = 0.0159\n",
      "epoch: 4680, loss = 0.0158\n",
      "epoch: 4700, loss = 0.0157\n",
      "epoch: 4720, loss = 0.0156\n",
      "epoch: 4740, loss = 0.0155\n",
      "epoch: 4760, loss = 0.0154\n",
      "epoch: 4780, loss = 0.0153\n",
      "epoch: 4800, loss = 0.0152\n",
      "epoch: 4820, loss = 0.0151\n",
      "epoch: 4840, loss = 0.0150\n",
      "epoch: 4860, loss = 0.0149\n",
      "epoch: 4880, loss = 0.0147\n",
      "epoch: 4900, loss = 0.0146\n",
      "epoch: 4920, loss = 0.0146\n",
      "epoch: 4940, loss = 0.0145\n",
      "epoch: 4960, loss = 0.0144\n",
      "epoch: 4980, loss = 0.0143\n",
      "epoch: 5000, loss = 0.0142\n",
      "epoch: 5020, loss = 0.0141\n",
      "epoch: 5040, loss = 0.0140\n",
      "epoch: 5060, loss = 0.0139\n",
      "epoch: 5080, loss = 0.0138\n",
      "epoch: 5100, loss = 0.0137\n",
      "epoch: 5120, loss = 0.0136\n",
      "epoch: 5140, loss = 0.0135\n",
      "epoch: 5160, loss = 0.0134\n",
      "epoch: 5180, loss = 0.0134\n",
      "epoch: 5200, loss = 0.0133\n",
      "epoch: 5220, loss = 0.0132\n",
      "epoch: 5240, loss = 0.0131\n",
      "epoch: 5260, loss = 0.0130\n",
      "epoch: 5280, loss = 0.0129\n",
      "epoch: 5300, loss = 0.0128\n",
      "epoch: 5320, loss = 0.0128\n",
      "epoch: 5340, loss = 0.0127\n",
      "epoch: 5360, loss = 0.0126\n",
      "epoch: 5380, loss = 0.0125\n",
      "epoch: 5400, loss = 0.0125\n",
      "epoch: 5420, loss = 0.0124\n",
      "epoch: 5440, loss = 0.0123\n",
      "epoch: 5460, loss = 0.0122\n",
      "epoch: 5480, loss = 0.0121\n",
      "epoch: 5500, loss = 0.0121\n",
      "epoch: 5520, loss = 0.0120\n",
      "epoch: 5540, loss = 0.0119\n",
      "epoch: 5560, loss = 0.0119\n",
      "epoch: 5580, loss = 0.0118\n",
      "epoch: 5600, loss = 0.0117\n",
      "epoch: 5620, loss = 0.0116\n",
      "epoch: 5640, loss = 0.0116\n",
      "epoch: 5660, loss = 0.0115\n",
      "epoch: 5680, loss = 0.0114\n",
      "epoch: 5700, loss = 0.0114\n",
      "epoch: 5720, loss = 0.0113\n",
      "epoch: 5740, loss = 0.0112\n",
      "epoch: 5760, loss = 0.0112\n",
      "epoch: 5780, loss = 0.0111\n",
      "epoch: 5800, loss = 0.0110\n",
      "epoch: 5820, loss = 0.0110\n",
      "epoch: 5840, loss = 0.0109\n",
      "epoch: 5860, loss = 0.0109\n",
      "epoch: 5880, loss = 0.0108\n",
      "epoch: 5900, loss = 0.0107\n",
      "epoch: 5920, loss = 0.0107\n",
      "epoch: 5940, loss = 0.0106\n",
      "epoch: 5960, loss = 0.0106\n",
      "epoch: 5980, loss = 0.0105\n",
      "epoch: 6000, loss = 0.0104\n",
      "epoch: 6020, loss = 0.0104\n",
      "epoch: 6040, loss = 0.0103\n",
      "epoch: 6060, loss = 0.0103\n",
      "epoch: 6080, loss = 0.0102\n",
      "epoch: 6100, loss = 0.0101\n",
      "epoch: 6120, loss = 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6140, loss = 0.0100\n",
      "epoch: 6160, loss = 0.0100\n",
      "epoch: 6180, loss = 0.0099\n",
      "epoch: 6200, loss = 0.0099\n",
      "epoch: 6220, loss = 0.0098\n",
      "epoch: 6240, loss = 0.0098\n",
      "epoch: 6260, loss = 0.0097\n",
      "epoch: 6280, loss = 0.0097\n",
      "epoch: 6300, loss = 0.0096\n",
      "epoch: 6320, loss = 0.0096\n",
      "epoch: 6340, loss = 0.0095\n",
      "epoch: 6360, loss = 0.0095\n",
      "epoch: 6380, loss = 0.0094\n",
      "epoch: 6400, loss = 0.0094\n",
      "epoch: 6420, loss = 0.0093\n",
      "epoch: 6440, loss = 0.0093\n",
      "epoch: 6460, loss = 0.0092\n",
      "epoch: 6480, loss = 0.0092\n",
      "epoch: 6500, loss = 0.0091\n",
      "epoch: 6520, loss = 0.0091\n",
      "epoch: 6540, loss = 0.0090\n",
      "epoch: 6560, loss = 0.0090\n",
      "epoch: 6580, loss = 0.0089\n",
      "epoch: 6600, loss = 0.0089\n",
      "epoch: 6620, loss = 0.0089\n",
      "epoch: 6640, loss = 0.0088\n",
      "epoch: 6660, loss = 0.0088\n",
      "epoch: 6680, loss = 0.0087\n",
      "epoch: 6700, loss = 0.0087\n",
      "epoch: 6720, loss = 0.0086\n",
      "epoch: 6740, loss = 0.0086\n",
      "epoch: 6760, loss = 0.0086\n",
      "epoch: 6780, loss = 0.0085\n",
      "epoch: 6800, loss = 0.0085\n",
      "epoch: 6820, loss = 0.0084\n",
      "epoch: 6840, loss = 0.0084\n",
      "epoch: 6860, loss = 0.0083\n",
      "epoch: 6880, loss = 0.0083\n",
      "epoch: 6900, loss = 0.0083\n",
      "epoch: 6920, loss = 0.0082\n",
      "epoch: 6940, loss = 0.0082\n",
      "epoch: 6960, loss = 0.0082\n",
      "epoch: 6980, loss = 0.0081\n",
      "epoch: 7000, loss = 0.0081\n",
      "epoch: 7020, loss = 0.0080\n",
      "epoch: 7040, loss = 0.0080\n",
      "epoch: 7060, loss = 0.0080\n",
      "epoch: 7080, loss = 0.0079\n",
      "epoch: 7100, loss = 0.0079\n",
      "epoch: 7120, loss = 0.0079\n",
      "epoch: 7140, loss = 0.0078\n",
      "epoch: 7160, loss = 0.0078\n",
      "epoch: 7180, loss = 0.0077\n",
      "epoch: 7200, loss = 0.0077\n",
      "epoch: 7220, loss = 0.0077\n",
      "epoch: 7240, loss = 0.0076\n",
      "epoch: 7260, loss = 0.0076\n",
      "epoch: 7280, loss = 0.0076\n",
      "epoch: 7300, loss = 0.0075\n",
      "epoch: 7320, loss = 0.0075\n",
      "epoch: 7340, loss = 0.0075\n",
      "epoch: 7360, loss = 0.0074\n",
      "epoch: 7380, loss = 0.0074\n",
      "epoch: 7400, loss = 0.0074\n",
      "epoch: 7420, loss = 0.0073\n",
      "epoch: 7440, loss = 0.0073\n",
      "epoch: 7460, loss = 0.0073\n",
      "epoch: 7480, loss = 0.0072\n",
      "epoch: 7500, loss = 0.0072\n",
      "epoch: 7520, loss = 0.0072\n",
      "epoch: 7540, loss = 0.0071\n",
      "epoch: 7560, loss = 0.0071\n",
      "epoch: 7580, loss = 0.0071\n",
      "epoch: 7600, loss = 0.0071\n",
      "epoch: 7620, loss = 0.0070\n",
      "epoch: 7640, loss = 0.0070\n",
      "epoch: 7660, loss = 0.0070\n",
      "epoch: 7680, loss = 0.0069\n",
      "epoch: 7700, loss = 0.0069\n",
      "epoch: 7720, loss = 0.0069\n",
      "epoch: 7740, loss = 0.0068\n",
      "epoch: 7760, loss = 0.0068\n",
      "epoch: 7780, loss = 0.0068\n",
      "epoch: 7800, loss = 0.0068\n",
      "epoch: 7820, loss = 0.0067\n",
      "epoch: 7840, loss = 0.0067\n",
      "epoch: 7860, loss = 0.0067\n",
      "epoch: 7880, loss = 0.0066\n",
      "epoch: 7900, loss = 0.0066\n",
      "epoch: 7920, loss = 0.0066\n",
      "epoch: 7940, loss = 0.0066\n",
      "epoch: 7960, loss = 0.0065\n",
      "epoch: 7980, loss = 0.0065\n",
      "epoch: 8000, loss = 0.0065\n",
      "epoch: 8020, loss = 0.0065\n",
      "epoch: 8040, loss = 0.0064\n",
      "epoch: 8060, loss = 0.0064\n",
      "epoch: 8080, loss = 0.0064\n",
      "epoch: 8100, loss = 0.0064\n",
      "epoch: 8120, loss = 0.0063\n",
      "epoch: 8140, loss = 0.0063\n",
      "epoch: 8160, loss = 0.0063\n",
      "epoch: 8180, loss = 0.0063\n",
      "epoch: 8200, loss = 0.0062\n",
      "epoch: 8220, loss = 0.0062\n",
      "epoch: 8240, loss = 0.0062\n",
      "epoch: 8260, loss = 0.0062\n",
      "epoch: 8280, loss = 0.0061\n",
      "epoch: 8300, loss = 0.0061\n",
      "epoch: 8320, loss = 0.0061\n",
      "epoch: 8340, loss = 0.0061\n",
      "epoch: 8360, loss = 0.0060\n",
      "epoch: 8380, loss = 0.0060\n",
      "epoch: 8400, loss = 0.0060\n",
      "epoch: 8420, loss = 0.0060\n",
      "epoch: 8440, loss = 0.0059\n",
      "epoch: 8460, loss = 0.0059\n",
      "epoch: 8480, loss = 0.0059\n",
      "epoch: 8500, loss = 0.0059\n",
      "epoch: 8520, loss = 0.0059\n",
      "epoch: 8540, loss = 0.0058\n",
      "epoch: 8560, loss = 0.0058\n",
      "epoch: 8580, loss = 0.0058\n",
      "epoch: 8600, loss = 0.0058\n",
      "epoch: 8620, loss = 0.0058\n",
      "epoch: 8640, loss = 0.0057\n",
      "epoch: 8660, loss = 0.0057\n",
      "epoch: 8680, loss = 0.0057\n",
      "epoch: 8700, loss = 0.0057\n",
      "epoch: 8720, loss = 0.0056\n",
      "epoch: 8740, loss = 0.0056\n",
      "epoch: 8760, loss = 0.0056\n",
      "epoch: 8780, loss = 0.0056\n",
      "epoch: 8800, loss = 0.0056\n",
      "epoch: 8820, loss = 0.0055\n",
      "epoch: 8840, loss = 0.0055\n",
      "epoch: 8860, loss = 0.0055\n",
      "epoch: 8880, loss = 0.0055\n",
      "epoch: 8900, loss = 0.0055\n",
      "epoch: 8920, loss = 0.0054\n",
      "epoch: 8940, loss = 0.0054\n",
      "epoch: 8960, loss = 0.0054\n",
      "epoch: 8980, loss = 0.0054\n",
      "epoch: 9000, loss = 0.0054\n",
      "epoch: 9020, loss = 0.0053\n",
      "epoch: 9040, loss = 0.0053\n",
      "epoch: 9060, loss = 0.0053\n",
      "epoch: 9080, loss = 0.0053\n",
      "epoch: 9100, loss = 0.0053\n",
      "epoch: 9120, loss = 0.0053\n",
      "epoch: 9140, loss = 0.0052\n",
      "epoch: 9160, loss = 0.0052\n",
      "epoch: 9180, loss = 0.0052\n",
      "epoch: 9200, loss = 0.0052\n",
      "epoch: 9220, loss = 0.0052\n",
      "epoch: 9240, loss = 0.0051\n",
      "epoch: 9260, loss = 0.0051\n",
      "epoch: 9280, loss = 0.0051\n",
      "epoch: 9300, loss = 0.0051\n",
      "epoch: 9320, loss = 0.0051\n",
      "epoch: 9340, loss = 0.0051\n",
      "epoch: 9360, loss = 0.0050\n",
      "epoch: 9380, loss = 0.0050\n",
      "epoch: 9400, loss = 0.0050\n",
      "epoch: 9420, loss = 0.0050\n",
      "epoch: 9440, loss = 0.0050\n",
      "epoch: 9460, loss = 0.0050\n",
      "epoch: 9480, loss = 0.0049\n",
      "epoch: 9500, loss = 0.0049\n",
      "epoch: 9520, loss = 0.0049\n",
      "epoch: 9540, loss = 0.0049\n",
      "epoch: 9560, loss = 0.0049\n",
      "epoch: 9580, loss = 0.0049\n",
      "epoch: 9600, loss = 0.0048\n",
      "epoch: 9620, loss = 0.0048\n",
      "epoch: 9640, loss = 0.0048\n",
      "epoch: 9660, loss = 0.0048\n",
      "epoch: 9680, loss = 0.0048\n",
      "epoch: 9700, loss = 0.0048\n",
      "epoch: 9720, loss = 0.0047\n",
      "epoch: 9740, loss = 0.0047\n",
      "epoch: 9760, loss = 0.0047\n",
      "epoch: 9780, loss = 0.0047\n",
      "epoch: 9800, loss = 0.0047\n",
      "epoch: 9820, loss = 0.0047\n",
      "epoch: 9840, loss = 0.0047\n",
      "epoch: 9860, loss = 0.0046\n",
      "epoch: 9880, loss = 0.0046\n",
      "epoch: 9900, loss = 0.0046\n",
      "epoch: 9920, loss = 0.0046\n",
      "epoch: 9940, loss = 0.0046\n",
      "epoch: 9960, loss = 0.0046\n",
      "epoch: 9980, loss = 0.0046\n",
      "epoch: 10000, loss = 0.0045\n"
     ]
    }
   ],
   "source": [
    "criterions = torch.nn.CrossEntropyLoss()\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jP7jtSy2Txz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.96      0.96      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.96      0.96      0.96       982\n",
      "           5       0.96      0.95      0.95       892\n",
      "           6       0.97      0.96      0.97       958\n",
      "           7       0.97      0.96      0.97      1028\n",
      "           8       0.96      0.95      0.96       974\n",
      "           9       0.95      0.96      0.95      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n",
    "    print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (5 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From a perceptron's view it has only a step function as its tranfer function, while logistic regression has the meaning related to probabilty behind.\n",
    "2. Advantage: No need for human feature extraction, easy to use and construct; Disadvantage: Requires long traning time, not always explainable.\n",
    "3. Two additive linear layers is equivalent to one linear layer, the activation function brings in non-linear tranforms which ensures the effect of multiple network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MindSpore \n",
    "\n",
    "![image-20221108120108735](https://s1.ax1x.com/2022/11/08/xxlb5R.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
